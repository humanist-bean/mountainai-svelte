# NOTE: This code was generated by ChatGPT
import torch
import tf2onnx
#import tensorflow as tf IMPORTHING THIS CAUSES A CONFLICT

# Step 1: Load the model.pkl file using fastai
model = torch.load("models/model.pkl")

# Step 2: Convert the model to ONNX format
# Replace input_size with the appropriate size for your model
dummy_input = torch.randn(1, input_size)
torch.onnx.export(model, dummy_input, "model.onnx")

# Step 3: Convert the ONNX model to TensorFlow format
tf_model = tf2onnx.convert.from_onnx_file("model.onnx")
tf2onnx.save_model(tf_model, "tf_model.onnx")

# Step 4: Convert the TensorFlow model to .tflite format
# NOTE: Moved to another because the flatbuffers versions that tf2onnx
# and tensorflow relied on are conflicting!
#converter = tf.lite.TFLiteConverter.from_saved_model("tf_model.onnx")
#tflite_model = converter.convert()
#open("model.tflite", "wb").write(tflite_model)
